{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2. CATEGORICAL ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUITE DU CHAPITRE 6.1. BASELINE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.1. COURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model (on the baseline data)\n",
    "train, valid, test = get_data_splits(data)\n",
    "train_model(train, valid)\n",
    "# Validation AUC score: 0.7467"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.1.1. CATEGORICAL COUNT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count encoding replaces each categorical value with the number of times \n",
    "# it appears in the dataset. For example, if the value \"GB\" occured 10 \n",
    "# times in the country feature, then each \"GB\" would be replaced with the \n",
    "# number 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the categorical-encodings package to get this encoding. \n",
    "# The encoder itself is available as CountEncoder. This encoder and the \n",
    "# others in categorical-encodings work like scikit-learn transformers \n",
    "# with .fit and .transform methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "\n",
    "# Create the encoder\n",
    "count_enc = ce.CountEncoder()\n",
    "\n",
    "# Transform the features, rename the columns with the _count suffix, \n",
    "# and join to dataframe\n",
    "count_encoded = count_enc.fit_transform(ks[cat_features])\n",
    "data = data.join(count_encoded.add_suffix(\"_count\"))\n",
    "\n",
    "# Train a model \n",
    "train, valid, test = get_data_splits(data)\n",
    "train_model(train, valid)\n",
    "# Validation AUC score: 0.7486\n",
    "# Adding the count encoding features increase the validation score from \n",
    "# 0.7467 to 0.7486, only a slight improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.1.2. TARGET ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding replaces a categorical value with the average value of \n",
    "# the target for that value of the feature. \n",
    "# For example, given the country value \"CA\", you'd calculate the average \n",
    "# outcome for all the rows with country == 'CA', around 0.28. T\n",
    "# This is often blended with the target probability over the entire dataset\n",
    "# to reduce the variance of values with few occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This technique uses the targets to create new features. \n",
    "# So including the validation or test data in the target encodings would \n",
    "# be a form of target leakage. \n",
    "# Instead, you should learn the target encodings from the training dataset \n",
    "# only and apply it to the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The category_encoders package provides TargetEncoder for target encoding.\n",
    "# The implementation is similar to CountEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "# Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "train_TE = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid_TE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
    "\n",
    "# Train a model\n",
    "train_model(train_TE, valid_TE)\n",
    "\n",
    "# Validation AUC score: 0.7491\n",
    "# The validation score is higher again, from 0.7467 to 0.7491."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.1.3. CATBOOST ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we'll look at CatBoost encoding. \n",
    "# This is similar to target encoding in that it's based on the target \n",
    "# probablity for a given value. \n",
    "# However with CatBoost, for each row, the target probability is calculated\n",
    "# only from the rows before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "target_enc = ce.CatBoostEncoder(cols=cat_features)\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "# Transform the features, rename columns with _cb suffix, and join to dataframe\n",
    "train_CBE = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
    "valid_CBE = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n",
    "\n",
    "# Train a model\n",
    "train_model(train_CBE, valid_CBE)\n",
    "\n",
    "# Validation AUC score: 0.7492\n",
    "# This does slightly better than target encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.2. EXERCICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset - fichier parquet\n",
    "clicks_path = 'C:/Users/PC Maison/4-KAGGLE/KAGGLE_DEV/KAGGLE_COURS_6-FEATURE_ENGINEERING/feature-engineering-data/input/'\n",
    "clicks = pd.read_parquet(clicks_path + 'baseline_data.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour spliter le dataset en TRAIN SET, VALID SET ET TEST SET\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    \"\"\"Splits a dataframe into train, validation, and test sets.\n",
    "\n",
    "    First, orders by the column 'click_time'. Set the size of the \n",
    "    validation and test sets with the valid_fraction keyword argument.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour entrainer scorer le modèle\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, \n",
    "             'objective': 'binary', \n",
    "             'metric': 'auc', \n",
    "             'seed': 7}\n",
    "    num_round = 1000\n",
    "    \n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model\n",
      "Validation AUC score: 0.9622743228943659\n"
     ]
    }
   ],
   "source": [
    "# baseline score :\n",
    "print(\"Baseline model\")\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2.1. CATEGORICAL ENCODING AND LEAKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ces encodages sont tous basés sur des statistiques calculées à partir de \n",
    "# l'ensemble de données comme les nombres et les moyennes.\n",
    "# Compte tenu de cela, quelles données devez-vous utiliser pour calculer \n",
    "# les encodages? \n",
    "# Plus précisément, pouvez-vous utiliser les données de validation? \n",
    "# Pouvez-vous utiliser les données de test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vous ne devez calculer les encodages qu'à partir du train set. \n",
    "# Si vous incluez les donnéesdu val set et du test set dans les encodages, \n",
    "# vous surestimerez les performances du modèle. \n",
    "# Vous devez en général être vigilant pour éviter les fuites, c'est-à-dire \n",
    "# inclure toute information provenant du valid set et du test set dans le\n",
    "# modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2.2. COUNT ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountEncoder from the category_encoders library, fit the encoding\n",
    "# using the categorical feature columns defined in cat_features.\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = ce.CountEncoder(cols=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Outils\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountEncoder(cols=['ip', 'app', 'device', 'os', 'channel'],\n",
       "             combine_min_nan_groups=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrainement du modèle\n",
    "# Learn encoding from the training set\n",
    "count_enc.fit(train[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then apply the encodings to the train and validation sets \n",
    "\n",
    "# Transform pour train set et valid set\n",
    "train_transf_count_enc = count_enc.transform(train[cat_features])\n",
    "valid_transf_count_enc = count_enc.transform(valid[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding them as new columns with names suffixed \"_count\".\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_count` as a suffix to the new columns\n",
    "train_encoded = train.join(train_transf_count_enc.add_suffix('_count'))\n",
    "valid_encoded = valid.join(valid_transf_count_enc.add_suffix('_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC score: 0.9653051135205329\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the encoded datasets\n",
    "# This can take around 30 seconds to complete\n",
    "_ = train_model(train_encoded, valid_encoded)\n",
    "# old_score : Validation AUC score: 0.9622743228943659\n",
    "# new_score : Validation AUC score: 0.9653051135205329\n",
    "# meilleur score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# À première vue, il peut être surprenant que l'encodage de comptage aide \n",
    "# à créer des modèles précis. \n",
    "# Pourquoi pensez-vous que l'encodage de comptage est une bonne idée ou \n",
    "# comment améliore-t-il le score du modèle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les valeurs rares ont tendance à avoir des nombres similaires (avec des \n",
    "# valeurs telles que 1 ou 2), vous pouvez donc classer les valeurs rares \n",
    "# ensemble au moment de la prédiction. \n",
    "# Il est peu probable que les valeurs courantes avec un grand nombre aient \n",
    "# le même nombre exact que les autres valeurs. \n",
    "# Ainsi, les valeurs communes / importantes obtiennent leur propre \n",
    "# groupement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.2.3. TARGET ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Outils\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# Create the target encoder. You can find this easily by using tab completion.\n",
    "# Start typing ce. the press Tab to bring up a list of classes and functions.\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "X_train = train[cat_features]\n",
    "y_train = train['is_attributed']\n",
    "X_valid = valid[cat_features]\n",
    "\n",
    "# Learn encoding from the training set. Use the 'is_attributed' column as the target.\n",
    "# fit sur X_train = train[cat_features], y_train = train['is_attributed']\n",
    "target_enc.fit(X_train, y_train)\n",
    "\n",
    "# Applying transform target encoding\n",
    "train_transf_target_enc = target_enc.transform(X_train)\n",
    "valid_transf_target_enc = target_enc.transform(X_valid)\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_target` as a suffix to the new columns\n",
    "train_encoded = train.join(train_transf_target_enc.add_suffix('_target'))\n",
    "valid_encoded = valid.join(valid_transf_target_enc.add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC score: 0.9540530347873288\n"
     ]
    }
   ],
   "source": [
    "# Run the next cell to see how target encoding affects your results.\n",
    "_ = train_model(train_encoded, valid_encoded)\n",
    "# old_score_1 : Validation AUC score: 0.9622743228943659\n",
    "# old_score_2 : Validation AUC score: 0.9653051135205329\n",
    "# new_score   : Validation AUC score: 0.9540530347873288\n",
    "# pas le meilleur score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essayez de supprimer le codage de la colonne I.P\n",
    "# Si vous laissez ip hors des fonctionnalités encodées et réentraînez le \n",
    "# modèle avec l'encodage cible, vous devriez constater que le score \n",
    "# augmente et est supérieur au score de base! \n",
    "# Pourquoi pensez-vous que le score est inférieur à la ligne de base \n",
    "# lorsque nous encodons l'adresse IP, mais supérieur à la ligne de base \n",
    "# lorsque nous ne le faisons pas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le codage cible tente de mesurer la moyenne de population de la cible \n",
    "# pour chaque niveau dans une fonction catégorielle. \n",
    "# Cela signifie que lorsqu'il y a moins de données par niveau, la moyenne \n",
    "# estimée sera plus éloignée de la moyenne «vraie», il y aura plus de \n",
    "# variance. \n",
    "# Il y a peu de données par adresse IP, il est donc probable que les \n",
    "# estimations soient beaucoup plus bruyantes que pour les autres \n",
    "# fonctionnalités. \n",
    "# Le modèle s'appuiera fortement sur cette fonctionnalité car elle est \n",
    "# extrêmement prédictive. \n",
    "# Cela l'amène à faire moins de fractionnements sur d'autres \n",
    "# fonctionnalités, et ces fonctionnalités sont adaptées uniquement aux \n",
    "# erreurs restantes concernant la comptabilisation de l'adresse IP. \n",
    "# Ainsi, le modèle fonctionnera très mal lorsqu'il verra de nouvelles \n",
    "# adresses IP qui ne figuraient pas dans les données d'entraînement \n",
    "# (ce qui est probablement la plupart des nouvelles données). \n",
    "# À l'avenir, nous laisserons de côté la fonction IP lors de l'essai de \n",
    "# différents encodages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.2.4. CATBOOST ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Outils\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# Remove IP from the encoded features\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "\n",
    "# Create the CatBoost encoder\n",
    "cb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=7)\n",
    "\n",
    "# Learn encoding from the training set\n",
    "X_train = train[cat_features]\n",
    "y_train = train['is_attributed']\n",
    "X_valid = valid[cat_features]\n",
    "cb_enc.fit(X_train, y_train)\n",
    "\n",
    "# Transformation par encodage du train set et valid set\n",
    "train_transf_c_enc = cb_enc.transform(X_train)\n",
    "valid_transf_c_enc = cb_enc.transform(X_valid)\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_cb` as a suffix to the new columns\n",
    "train_encoded = train.join(train_transf_c_enc.add_suffix('_cb'))\n",
    "valid_encoded = valid.join(valid_transf_c_enc.add_suffix('_cb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC score: 0.962868024575231\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train_encoded, valid_encoded)\n",
    "# old_score_1 : Validation AUC score: 0.9622743228943659\n",
    "# old_score_2 : Validation AUC score: 0.9653051135205329\n",
    "# old_score_3 : Validation AUC score: 0.9540530347873288\n",
    "# new_score   : Validation AUC score: 0.962868024575231  \n",
    "# meilleur score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
