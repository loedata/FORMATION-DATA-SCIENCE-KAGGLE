{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3. FEATURE GENERATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Outils\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Charger un fichier csv\n",
    "kickstarter_path = 'C:/Users/PC Maison/4-KAGGLE/KAGGLE_DEV/KAGGLE_COURS_6-FEATURE_ENGINEERING/kickstarter-projects/input/'\n",
    "kickstarter = pd.read_csv(kickstarter_path + 'ks-projects-201612.csv',\n",
    "                 parse_dates=True,\n",
    "                 encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'name', 'category', 'main_category', 'currency', 'deadline',\n",
       "       'goal', 'launched', 'pledged', 'state', 'backers', 'country',\n",
       "       'usd pledged', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15',\n",
       "       'Unnamed: 16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppression de l'espace après le nom des colonnes\n",
    "ks = kickstarter.rename(columns={'ID ': 'ID',\n",
    "                        'name ': 'name',\n",
    "                        'category ': 'category',\n",
    "                        'main_category ': 'main_category',\n",
    "                        'currency ': 'currency',\n",
    "                        'deadline ': 'deadline',\n",
    "                        'goal ': 'goal',\n",
    "                        'launched ': 'launched',\n",
    "                        'pledged ': 'pledged',\n",
    "                        'state ': 'state',\n",
    "                        'backers ': 'backers',\n",
    "                        'country ': 'country',\n",
    "                        'usd pledged ': 'usd pledged'})\n",
    "ks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'name', 'category', 'main_category', 'currency', 'deadline',\n",
       "       'goal', 'launched', 'pledged', 'state', 'backers', 'country',\n",
       "       'usd pledged'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supprimer les colonnes 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15',\n",
    "# 'Unnamed: 16'\n",
    "ks.drop(['Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16'], \n",
    "          axis=1, inplace=True)\n",
    "ks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                int64\n",
       "name             object\n",
       "category         object\n",
       "main_category    object\n",
       "currency         object\n",
       "deadline         object\n",
       "goal             object\n",
       "launched         object\n",
       "pledged          object\n",
       "state            object\n",
       "backers          object\n",
       "country          object\n",
       "usd pledged      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>goal</th>\n",
       "      <th>launched</th>\n",
       "      <th>pledged</th>\n",
       "      <th>state</th>\n",
       "      <th>backers</th>\n",
       "      <th>country</th>\n",
       "      <th>usd pledged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002330</td>\n",
       "      <td>The Songs of Adelaide &amp; Abullah</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09 11:36:00</td>\n",
       "      <td>1000</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>0</td>\n",
       "      <td>failed</td>\n",
       "      <td>0</td>\n",
       "      <td>GB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000004038</td>\n",
       "      <td>Where is Hank?</td>\n",
       "      <td>Narrative Film</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26 00:20:50</td>\n",
       "      <td>45000</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>220</td>\n",
       "      <td>failed</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000007540</td>\n",
       "      <td>ToshiCapital Rekordz Needs Help to Complete Album</td>\n",
       "      <td>Music</td>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16 04:24:11</td>\n",
       "      <td>5000</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>1</td>\n",
       "      <td>failed</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000011046</td>\n",
       "      <td>Community Film Project: The Art of Neighborhoo...</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29 01:00:00</td>\n",
       "      <td>19500</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>1283</td>\n",
       "      <td>canceled</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "      <td>1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000014025</td>\n",
       "      <td>Monarch Espresso Bar</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-04-01 13:38:27</td>\n",
       "      <td>50000</td>\n",
       "      <td>2016-02-26 13:38:27</td>\n",
       "      <td>52375</td>\n",
       "      <td>successful</td>\n",
       "      <td>224</td>\n",
       "      <td>US</td>\n",
       "      <td>52375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               name  \\\n",
       "0  1000002330                    The Songs of Adelaide & Abullah   \n",
       "1  1000004038                                     Where is Hank?   \n",
       "2  1000007540  ToshiCapital Rekordz Needs Help to Complete Album   \n",
       "3  1000011046  Community Film Project: The Art of Neighborhoo...   \n",
       "4  1000014025                               Monarch Espresso Bar   \n",
       "\n",
       "         category main_category currency             deadline   goal  \\\n",
       "0          Poetry    Publishing      GBP  2015-10-09 11:36:00   1000   \n",
       "1  Narrative Film  Film & Video      USD  2013-02-26 00:20:50  45000   \n",
       "2           Music         Music      USD  2012-04-16 04:24:11   5000   \n",
       "3    Film & Video  Film & Video      USD  2015-08-29 01:00:00  19500   \n",
       "4     Restaurants          Food      USD  2016-04-01 13:38:27  50000   \n",
       "\n",
       "              launched pledged       state backers country usd pledged  \n",
       "0  2015-08-11 12:12:28       0      failed       0      GB           0  \n",
       "1  2013-01-12 00:20:50     220      failed       3      US         220  \n",
       "2  2012-03-17 03:24:11       1      failed       1      US           1  \n",
       "3  2015-07-04 08:35:03    1283    canceled      14      US        1283  \n",
       "4  2016-02-26 13:38:27   52375  successful     224      US       52375  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.1. COURS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1.1. INTERACTIONS ENTRE CATEGORICAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de nouvelles variables dans le dataset en combinat les\n",
    "# catégorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Poetry_GB\n",
      "1    Narrative Film_US\n",
      "2             Music_US\n",
      "3      Film & Video_US\n",
      "4       Restaurants_US\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# utilisation du broadcating de pandas\n",
    "interactions = ks['category'] + \"_\" + ks['country']\n",
    "print(interactions.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d874aeffcf47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabel_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_interaction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_country\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata_interaction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "data_interaction = baseline_data.assign(category_country=label_enc.fit_transform(interactions))\n",
    "data_interaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1.2. Number of projects in the last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuite, nous compterons le nombre de projets lancés la semaine \n",
    "# précédente pour chaque enregistrement. \n",
    "# J'utiliserai la méthode .rolling sur une série avec la colonne launched \n",
    "# comme index. \n",
    "# Je vais créer la série, en utilisant ks.launched comme index et ks.index \n",
    "# comme valeurs, puis trier les heures. \n",
    "# L'utilisation d'une série chronologique comme indice nous permet de \n",
    "# définir la taille de la fenêtre glissante en termes d'heures, de jours, \n",
    "# de semaines, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a Series with a timestamp index\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "launched.head(20)\n",
    "# launched\n",
    "# 1970-01-01 01:00:00     94579\n",
    "# 1970-01-01 01:00:00    319002\n",
    "# 1970-01-01 01:00:00    247913\n",
    "# 1970-01-01 01:00:00     48147\n",
    "# 1970-01-01 01:00:00     75397\n",
    "# 1970-01-01 01:00:00      2842\n",
    "# 1970-01-01 01:00:00    273779\n",
    "# 2009-04-21 21:02:48    169268\n",
    "# 2009-04-23 00:07:53    322000\n",
    "# 2009-04-24 21:52:03    138572\n",
    "# 2009-04-25 17:36:21    325391\n",
    "# 2009-04-27 14:10:39    122662\n",
    "# 2009-04-28 13:55:41    213711\n",
    "# 2009-04-29 02:04:21    345606\n",
    "# 2009-04-29 02:58:50    235255\n",
    "# 2009-04-29 04:37:37     98954\n",
    "# 2009-04-29 05:26:32    342226\n",
    "# 2009-04-29 06:43:44    275091\n",
    "# 2009-04-29 13:52:03    284115\n",
    "# 2009-04-29 22:08:13     32898\n",
    "# Name: count_7_days, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il y a sept projets qui ont manifestement des dates de lancement erronées\n",
    "# mais nous allons simplement les ignorer. Encore une fois, c'est quelque \n",
    "# chose que vous gérerez lors du nettoyage des données, mais ce n'est pas \n",
    "# l'objet de ce mini-cours.\n",
    "# Avec un index de séries chronologiques, vous pouvez utiliser .rolling \n",
    "# pour sélectionner des périodes comme fenêtre. \n",
    "# Par exemple, launch.rolling ('7d') crée une fenêtre déroulante contenant \n",
    "# toutes les données des 7 jours précédents. \n",
    "# La fenêtre contient l'enregistrement en cours, donc si nous voulons \n",
    "# compter tous les projets précédents mais pas celui en cours, \n",
    "# nous devrons soustraire 1. \n",
    "# Nous allons tracer les résultats pour nous assurer qu'il a l'air correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "print(count_7_days.head(20))\n",
    "\n",
    "# Ignore records with broken launch dates\n",
    "plt.plot(count_7_days[7:]);\n",
    "plt.title(\"Number of projects launched over periods of 7 days\");\n",
    "# launched\n",
    "# 1970-01-01 01:00:00     0.0\n",
    "# 1970-01-01 01:00:00     1.0\n",
    "# 1970-01-01 01:00:00     2.0\n",
    "# 1970-01-01 01:00:00     3.0\n",
    "# 1970-01-01 01:00:00     4.0\n",
    "# 1970-01-01 01:00:00     5.0\n",
    "# 1970-01-01 01:00:00     6.0\n",
    "# 2009-04-21 21:02:48     0.0\n",
    "# 2009-04-23 00:07:53     1.0\n",
    "# 2009-04-24 21:52:03     2.0\n",
    "# 2009-04-25 17:36:21     3.0\n",
    "# 2009-04-27 14:10:39     4.0\n",
    "# 2009-04-28 13:55:41     5.0\n",
    "# 2009-04-29 02:04:21     5.0\n",
    "# 2009-04-29 02:58:50     6.0\n",
    "# 2009-04-29 04:37:37     7.0\n",
    "# 2009-04-29 05:26:32     8.0\n",
    "# 2009-04-29 06:43:44     9.0\n",
    "# 2009-04-29 13:52:03    10.0\n",
    "# 2009-04-29 22:08:13    11.0\n",
    "# Name: count_7_days, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant que nous avons les décomptes, nous devons ajuster l'index afin\n",
    "# de pouvoir le joindre aux autres données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days.head(10)\n",
    "# 0    1409.0\n",
    "# 1     957.0\n",
    "# 2     739.0\n",
    "# 3     907.0\n",
    "# 4    1429.0\n",
    "# 5    1284.0\n",
    "# 6    1119.0\n",
    "# 7    1391.0\n",
    "# 8    1043.0\n",
    "# 9    3199.0\n",
    "# Name: count_7_days, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoignez à nouveau la nouvelle features avec les autres données en \n",
    "# utilisant .join puisque nous avons fait correspondre l'index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data.join(count_7_days).head(10)\n",
    "# \tgoal\thour\tday\tmonth\tyear\toutcome\tcategory\tcurrency\tcountry\tcount_7_days\n",
    "# 0\t1000.0\t12\t11\t8\t2015\t0\t108\t5\t9\t1409.0\n",
    "# 1\t30000.0\t4\t2\t9\t2017\t0\t93\t13\t22\t957.0\n",
    "# 2\t45000.0\t0\t12\t1\t2013\t0\t93\t13\t22\t739.0\n",
    "# 3\t5000.0\t3\t17\t3\t2012\t0\t90\t13\t22\t907.0\n",
    "# 4\t19500.0\t8\t4\t7\t2015\t0\t55\t13\t22\t1429.0\n",
    "# 5\t50000.0\t13\t26\t2\t2016\t1\t123\t13\t22\t1284.0\n",
    "# 6\t1000.0\t18\t1\t12\t2014\t1\t58\t13\t22\t1119.0\n",
    "# 7\t25000.0\t20\t1\t2\t2016\t0\t41\t13\t22\t1391.0\n",
    "# 8\t125000.0\t18\t24\t4\t2014\t0\t113\t13\t22\t1043.0\n",
    "# 9\t65000.0\t21\t11\t7\t2014\t0\t39\t13\t22\t3199.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do projects in the same category compete for donors? If you're trying to \n",
    "# fund a video game and another game project was just launched, you might \n",
    "# not get as much money. We can capture this by calculating the time since \n",
    "# the last launch project in the same category.\n",
    "# A handy method for performing operations within groups is to use .groupby\n",
    "# then .transform. The .transform method takes a function then passes a \n",
    "# series or dataframe to that function for each group. \n",
    "# This returns a dataframe with the same indices as the original dataframe.\n",
    "# In our case, we'll perform a groupby on \"category\" and use transform to \n",
    "# calculate the time differences for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600.\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project)\n",
    "timedeltas.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get NaNs here for projects that are the first in their category. \n",
    "# We'll need to fill those in with something like the mean or median. \n",
    "# We'll also need to reset the index so we can join it with the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final time since last project\n",
    "timedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index)\n",
    "timedeltas.head(20)\n",
    "# \tlaunched\n",
    "# 0\t18.606111\n",
    "# 1\t5.592778\n",
    "# 2\t1.313611\n",
    "# 3\t0.635000\n",
    "# 4\t16.661389\n",
    "# 5\t2.629722\n",
    "# 6\t0.367500\n",
    "# 7\t12.286111\n",
    "# 8\t14.243611\n",
    "# 9\t0.174722\n",
    "# 10\t1.372222\n",
    "# 11\t8.524444\n",
    "# 12\t0.015833\n",
    "# 13\t9.884444\n",
    "# 14\t1.725556\n",
    "# 15\t3.806111\n",
    "# 16\t2.654167\n",
    "# 17\t26.531667\n",
    "# 18\t12.273611\n",
    "# 19\t9.288889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1.3. TRANSFORMING NUMERICAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of the values in \"goal\" shows that most projects have \n",
    "# goals less than 5000 USD.\n",
    "plt.hist(ks.goal, range=(0, 100000), bins=50);\n",
    "plt.title('Goal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sqrt(ks.goal), range=(0, 400), bins=50);\n",
    "plt.title('Sqrt(Goal)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(ks.goal), range=(0, 25), bins=50);\n",
    "plt.title('Log(Goal)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The log transformation won't help our model since tree-based models are \n",
    "# scale invariant. \n",
    "# However, this should help if we had a linear model or neural network.\n",
    "# Other transformations include squares and other powers, exponentials, etc\n",
    "# These might help the model discriminate, like the kernel trick for SVMs. \n",
    "# Again, it takes a bit of experimentation to see what works. \n",
    "# One method is to create a bunch of new features and later choose the \n",
    "# best ones with feature selection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.2. EXERCICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features from   timestamps\n",
    "click_path = 'C:/Users/PC Maison/4-KAGGLE/KAGGLE_DEV/KAGGLE_COURS_6-FEATURE_ENGINEERING/feature-engineering-data/input/'\n",
    "click_data = pd.read_csv(click_path + 'train_sample.csv', \n",
    "                         parse_dates=['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des dates / timestamps\n",
    "click_times = click_data['click_time']\n",
    "clicks = click_data.assign(day=click_times.dt.day.astype('uint8'),\n",
    "                           hour=click_times.dt.hour.astype('uint8'),\n",
    "                           minute=click_times.dt.minute.astype('uint8'),\n",
    "                           second=click_times.dt.second.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical features\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "for feature in cat_features:\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    clicks[feature] = label_encoder.fit_transform(clicks[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de split du dataset en TRAIN SET, VALID SET ET TEST SET    \n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entrainement et de scoring\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model. Hold on a minute to see the validation score\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model score\n",
      "Training model. Hold on a minute to see the validation score\n",
      "Validation AUC score: 0.9622743228943659\n"
     ]
    }
   ],
   "source": [
    "# Test du modèle\n",
    "print(\"Baseline model score\")\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)\n",
    "# Baseline model score\n",
    "# Training model. Hold on a minute to see the validation score\n",
    "# Validation AUC score: 0.9622743228943659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.2.1. ADD INTERACTION FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you'll add interaction features for each pair of categorical \n",
    "# features (ip, app, device, os, channel). \n",
    "# The easiest way to iterate through the pairs of features is with \n",
    "# itertools.combinations. \n",
    "# For each new column, join the values as strings with an underscore, \n",
    "# so 13 and 47 would become \"13_47\". \n",
    "# As you add the new columns to the dataset, be sure to label encode the \n",
    "# values.\n",
    "\n",
    "import itertools\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "interactions = pd.DataFrame(index=clicks.index)\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "    new_col_name = '_'.join([col1, col2])\n",
    "    \n",
    "    # Convert to strings and combine\n",
    "    new_values = clicks[col1].map(str) + \"_\" + clicks[col2].map(str)\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    interactions[new_col_name] = encoder.fit_transform(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with interactions\n",
      "Training model. Hold on a minute to see the validation score\n",
      "Validation AUC score: 0.9626212895350978\n"
     ]
    }
   ],
   "source": [
    "clicks = clicks.join(interactions)\n",
    "print(\"Score with interactions\")\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)\n",
    "# Score with interactions\n",
    "# Training model. Hold on a minute to see the validation score\n",
    "# old_score_1 : Validation AUC score: 0.9622743228943659\n",
    "# new_score_1 : Validation AUC score: 0.9626212895350978"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.2.2. GENERATING NUMERICAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interactions is a quick way to create more categorical features \n",
    "# from the data. It's also effective to create new numerical features, \n",
    "# you'll typically get a lot of improvement in the model. \n",
    "# This takes a bit of brainstorming and experimentation to find features \n",
    "# that work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2.2.1. Number of events in the past 6 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first feature you'll be creating is the number of events from the \n",
    "# same IP in the last six hours. It's likely that someone who is visiting \n",
    "# often will download the app.\n",
    "# Implement a function count_past_events that takes a Series of click times\n",
    "# (timestamps) and returns another Series with the number of events in the \n",
    "# last six hours. Tip: The rolling method is useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_past_events(series):\n",
    "    series = pd.Series(series.index, index=series)\n",
    "    # Subtract 1 so the current event isn't counted\n",
    "    past_events = series.rolling('6h').count() - 1\n",
    "    \n",
    "    return past_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model. Hold on a minute to see the validation score\n",
      "Validation AUC score: 0.9647255487084245\n"
     ]
    }
   ],
   "source": [
    "# Loading in from saved Parquet file\n",
    "past_events = pd.read_parquet(click_path + 'past_6hr_events.pqt')\n",
    "clicks['ip_past_6hr_counts'] = past_events\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)\n",
    "# Training model. Hold on a minute to see the validation score\n",
    "# old_score_1 : Validation AUC score: 0.9622743228943659\n",
    "# old_score_2 : Validation AUC score: 0.9626212895350978\n",
    "# new_score_2 : Validation AUC score: 0.9647255487084245"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2.2.2. FEATURES FROM THE FUTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, you shouldn't use information from the future. \n",
    "# When you're using models like this in a real-world scenario you won't \n",
    "# have data from the future. \n",
    "# Your model's score will likely be higher when training and testing on \n",
    "# historical data, but it will overestimate the performance on real data. \n",
    "# I should note that using future data will improve the score on Kaggle \n",
    "# competition test data, but avoid it when building machine learning \n",
    "# products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2.2.3. TIME SINCE LAST EVENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function time_diff that calculates the time since the last \n",
    "# event in seconds from a Series of timestamps. \n",
    "# This will be ran like so:\n",
    "# timedeltas = clicks.groupby('ip')['click_time'].transform(time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(series):\n",
    "    \"\"\"Returns a series with the time since the last timestamp \n",
    "       in seconds.\"\"\"\n",
    "    return series.diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model. Hold on a minute to see the validation score\n",
      "Validation AUC score: 0.9651116624672765\n"
     ]
    }
   ],
   "source": [
    "# Loading in from saved Parquet file\n",
    "past_events = pd.read_parquet(click_path + 'time_deltas.pqt')\n",
    "clicks['past_events_6hr'] = past_events\n",
    "\n",
    "train, valid, test = get_data_splits(clicks.join(past_events))\n",
    "_ = train_model(train, valid)\n",
    "# Training model. Hold on a minute to see the validation score\n",
    "# old_score_1 : Validation AUC score: 0.9622743228943659\n",
    "# old_score_2 : Validation AUC score: 0.9626212895350978\n",
    "# old_score_3 : Validation AUC score: 0.9647255487084245\n",
    "# new_score_3 : Validation AUC score: 0.9651116624672765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2.2.4. NUMBER OF PREVIOUS APP DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's likely that if a visitor downloaded an app previously, it'll affect \n",
    "# the likelihood they'll download one again. Implement a function \n",
    "# previous_attributions that returns a Series with the number of times an \n",
    "# app has been downloaded ('is_attributed' == 1) before the current event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_attributions(series):\n",
    "    # Subtracting raw values so I don't count the current event\n",
    "    sums = series.expanding(min_periods=2).sum() - series\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model. Hold on a minute to see the validation score\n",
      "Validation AUC score: 0.965236652054989\n"
     ]
    }
   ],
   "source": [
    "# Loading in from saved Parquet file\n",
    "past_events = pd.read_parquet(click_path + 'downloads.pqt')\n",
    "clicks['ip_past_6hr_counts'] = past_events\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)\n",
    "# old_score_1 : Validation AUC score: 0.9622743228943659\n",
    "# old_score_2 : Validation AUC score: 0.9626212895350978\n",
    "# old_score_3 : Validation AUC score: 0.9647255487084245\n",
    "# old_score_3 : Validation AUC score: 0.9651116624672765\n",
    "# new_score_4 : Validation AUC score: 0.965236652054989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2.2.5. TREE-BASED VERSUS NEURAL NETWORK MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we've been using LightGBM, a tree-based model. Would these \n",
    "# features we've generated work well for neural networks as well as \n",
    "# tree-based models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features themselves will work for either model. However, numerical \n",
    "# inputs to neural networks need to be standardized first. \n",
    "# That is, the features need to be scaled such that they have 0 mean and a \n",
    "# standard deviation of 1. This can be done using sklearn.preprocessing.\n",
    "# StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that you've generated a bunch of different features, you'll typically\n",
    "# want to remove some of them to reduce the size of the model and \n",
    "# potentially improve the performance. \n",
    "# Next, I'll show you how to do feature selection using a few different \n",
    "# methods such as L1 regression and Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
