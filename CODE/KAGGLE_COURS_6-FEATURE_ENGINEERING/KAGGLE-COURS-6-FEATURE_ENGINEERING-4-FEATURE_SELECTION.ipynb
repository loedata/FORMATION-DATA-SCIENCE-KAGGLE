{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4. FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUT : réduire le nombre de variables pour rendre le modèle plus \n",
    "# performant : avec la technique des FEATURES SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.1. COURS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.1.1. UNIVARIATE FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest and fastest methods are based on univariate statistical \n",
    "# tests. For each feature, measure how strongly the target depends on the \n",
    "# feature using a statistical test like  χ2  or ANOVA.\n",
    "# \n",
    "# From the scikit-learn feature selection module, \n",
    "# feature_selection.SelectKBest returns the K best features given some \n",
    "# scoring function. \n",
    "# For our classification problem, the module provides three different \n",
    "# scoring functions:  χ2 , ANOVA F-value, and the mutual information score. The F-value measures the linear dependency between the feature variable and the target. This means the score might underestimate the relation between a feature and the target if the relationship is nonlinear. The mutual information score is nonparametric and so can capture nonlinear relationships.\n",
    "# \n",
    "# With SelectKBest, we define the number of features to keep, based on the\n",
    "# score from the scoring function. Using .fit_transform(features, target) \n",
    "# we get back an array with only the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(baseline_data[feature_cols], baseline_data['outcome'])\n",
    "X_new\n",
    "# array([[2015.,    5.,    9.,   18., 1409.],\n",
    "#        [2017.,   13.,   22.,   31.,  957.],\n",
    "#        [2013.,   13.,   22.,   31.,  739.],\n",
    "#        ...,\n",
    "#        [2010.,   13.,   22.,   31.,  238.],\n",
    "#        [2016.,   13.,   22.,   31., 1100.],\n",
    "#        [2011.,   13.,   22.,   31.,  542.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, I've done something wrong here. \n",
    "# The statistical tests are calculated using all of the data. \n",
    "# This means information from the validation and test sets could influence\n",
    "# the features we keep, introducing a source of leakage. \n",
    "# This means we should select features using only a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = baseline_data.columns.drop('outcome')\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "# Keep 5 features\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "\n",
    "X_new = selector.fit_transform(train[feature_cols], train['outcome'])\n",
    "X_new\n",
    "# array([[2.015e+03, 5.000e+00, 9.000e+00, 1.800e+01, 1.409e+03],\n",
    "#        [2.017e+03, 1.300e+01, 2.200e+01, 3.100e+01, 9.570e+02],\n",
    "#        [2.013e+03, 1.300e+01, 2.200e+01, 3.100e+01, 7.390e+02],\n",
    "#        ...,\n",
    "#        [2.011e+03, 1.300e+01, 2.200e+01, 3.100e+01, 5.150e+02],\n",
    "#        [2.015e+03, 1.000e+00, 3.000e+00, 2.000e+00, 1.306e+03],\n",
    "#        [2.013e+03, 1.300e+01, 2.200e+01, 3.100e+01, 1.084e+03]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should notice that the selected features are different than when I \n",
    "# used the entire dataset. \n",
    "# Now we have our selected features, but it's only the feature values for \n",
    "# the training set. \n",
    "# To drop the rejected features from the validation and test sets, we need \n",
    "# to figure out which columns in the dataset were kept with SelectKBest. \n",
    "# To do this, we can use .inverse_transform to get back an array with the \n",
    "# shape of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=train.index, \n",
    "                                 columns=feature_cols)\n",
    "selected_features.head()\n",
    "# \tgoal\thour\tday\tmonth\tyear\tcategory\tcurrency\tcountry\tcategory_currency\tcategory_country\tcurrency_country\tcount_7_days\ttime_since_last_project\n",
    "# 0\t0.0\t0.0\t0.0\t0.0\t2015.0\t0.0\t5.0\t9.0\t0.0\t0.0\t18.0\t1409.0\t0.0\n",
    "# 1\t0.0\t0.0\t0.0\t0.0\t2017.0\t0.0\t13.0\t22.0\t0.0\t0.0\t31.0\t957.0\t0.0\n",
    "# 2\t0.0\t0.0\t0.0\t0.0\t2013.0\t0.0\t13.0\t22.0\t0.0\t0.0\t31.0\t739.0\t0.0\n",
    "# 3\t0.0\t0.0\t0.0\t0.0\t2012.0\t0.0\t13.0\t22.0\t0.0\t0.0\t31.0\t907.0\t0.0\n",
    "# 4\t0.0\t0.0\t0.0\t0.0\t2015.0\t0.0\t13.0\t22.0\t0.0\t0.0\t31.0\t1429.0\t0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a DataFrame with the same index and columns as the training \n",
    "# set, but all the dropped columns are filled with zeros. \n",
    "# We can find the selected columns by choosing features where the variance\n",
    "# is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped columns have values of all 0s, so var is 0, drop them\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "# Get the valid dataset with the selected features.\n",
    "valid[selected_columns].head()\n",
    "# \tyear\tcurrency\tcountry\tcurrency_country\tcount_7_days\n",
    "# 302896\t2015\t13\t22\t31\t1534.0\n",
    "# 302897\t2013\t13\t22\t31\t625.0\n",
    "# 302898\t2014\t5\t9\t18\t851.0\n",
    "# 302899\t2014\t13\t22\t31\t1973.0\n",
    "# 302900\t2014\t5\t9\t18\t2163.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.1.2. L1 REGULARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate methods consider only one feature at a time when making a \n",
    "# selection decision. \n",
    "# Instead, we can make our selection using all of the features by including\n",
    "# them in a linear model with L1 regularization. \n",
    "# This type of regularization (sometimes called Lasso) penalizes the \n",
    "# absolute magnitude of the coefficients, as compared to L2 (Ridge) \n",
    "# regression which penalizes the square of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the strength of regularization is increased, features which are less \n",
    "# important for predicting the target are set to 0. \n",
    "# This allows us to perform feature selection by adjusting the \n",
    "# regularization parameter. We choose the parameter by finding the best \n",
    "# performance on a hold-out set, or decide ahead of time how many features\n",
    "# to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression problems you can use sklearn.linear_model.Lasso, or \n",
    "# sklearn.linear_model.LogisticRegression for classification. \n",
    "# These can be used along with sklearn.feature_selection.SelectFromModel \n",
    "# to select the non-zero coefficients. \n",
    "# Otherwise, the code is similar to the univariate tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "\n",
    "X, y = train[train.columns.drop(\"outcome\")], train['outcome']\n",
    "\n",
    "# Set the regularization parameter C=1\n",
    "logistic = LogisticRegression(C=1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X, y)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X)\n",
    "X_new\n",
    "# array([[1.000e+03, 1.200e+01, 1.100e+01, ..., 1.900e+03, 1.800e+01,\n",
    "#         1.409e+03],\n",
    "#        [3.000e+04, 4.000e+00, 2.000e+00, ..., 1.630e+03, 3.100e+01,\n",
    "#         9.570e+02],\n",
    "#        [4.500e+04, 0.000e+00, 1.200e+01, ..., 1.630e+03, 3.100e+01,\n",
    "#         7.390e+02],\n",
    "#        ...,\n",
    "#        [2.500e+03, 0.000e+00, 3.000e+00, ..., 1.830e+03, 3.100e+01,\n",
    "#         5.150e+02],\n",
    "#        [2.600e+03, 2.100e+01, 2.300e+01, ..., 1.036e+03, 2.000e+00,\n",
    "#         1.306e+03],\n",
    "#        [2.000e+04, 1.600e+01, 4.000e+00, ..., 9.200e+02, 3.100e+01,\n",
    "#         1.084e+03]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the univariate tests, we get back an array with the selected \n",
    "# features. \n",
    "# Again, we will want to convert these to a DataFrame so we can get the \n",
    "# selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X.index,\n",
    "                                 columns=X.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case with the L1 parameter C=1, we're dropping the \n",
    "# time_since_last_project column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, feature selection with L1 regularization is more powerful \n",
    "# the univariate tests, but it can also be very slow when you have a lot \n",
    "# of data and a lot of features. \n",
    "# Univariate tests will be much faster on large datasets, but also will \n",
    "# likely perform worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2. EXERCICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargmenent du dataset\n",
    "clicks_path='C:/Users/PC Maison/4-KAGGLE/KAGGLE_DEV/KAGGLE_COURS_6-FEATURE_ENGINEERING/feature-engineering-data/input/'\n",
    "clicks = pd.read_parquet(clicks_path + 'baseline_data.pqt')\n",
    "data_files = ['count_encodings.pqt',\n",
    "              'catboost_encodings.pqt',\n",
    "              'interactions.pqt',\n",
    "              'past_6hr_events.pqt',\n",
    "              'downloads.pqt',\n",
    "              'time_deltas.pqt',\n",
    "              'svd_encodings.pqt']\n",
    "data_root = clicks_path\n",
    "for file in data_files:\n",
    "    features = pd.read_parquet(os.path.join(data_root, file))\n",
    "    clicks = clicks.join(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de splits du TRAIN SET, VALID SET ET TEST SET\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entrainement et de scoring du model\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9658334271834417\n"
     ]
    }
   ],
   "source": [
    "# BASELINE SCORE\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_, baseline_score = train_model(train, valid)\n",
    "# new_score_1 : Validation AUC score: 0.9658334271834417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2.1. WHICH DATA TO USE FOR FEATURE SELECTION?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since many feature selection methods require calculating statistics from \n",
    "# the dataset, should you use all the data for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including validation and test data within the feature selection is a \n",
    "# source of leakage. \n",
    "# You'll want to perform feature selection on the train set only, then use \n",
    "# the results there to remove features from the validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2.2. UNIVARIATE FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time',\n",
    "                                    'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "X_train=train[feature_cols]\n",
    "y_train=train['is_attributed']\n",
    "X_valid=valid[feature_cols]\n",
    "\n",
    "# Create the selector, keeping 40 features\n",
    "selector = SelectKBest(f_classif, k=40)\n",
    "\n",
    "# Use the selector to retrieve the best features fit_transform\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new),\n",
    "                                 index=train.index,\n",
    "                                 columns=feature_cols)\n",
    "\n",
    "# Find the columns that were dropped\n",
    "dropped_columns = selected_features.columns[selected_features.var() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9625481759576047\n"
     ]
    }
   ],
   "source": [
    "# Entrainement et scoring\n",
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1))\n",
    "# old_score_1 : Validation AUC score: 0.9658334271834417\n",
    "# new_score_2 : Validation AUC score: 0.9625481759576047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2.3. THE BEST VALUE OF K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  With this method we can choose the best K features, but we still have \n",
    "# to choose K ourselves. How would you find the \"best\" value of K? \n",
    "# That is, you want it to be small so you're keeping the best features, \n",
    "# but not so small that it's degrading the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the best value of K, you can fit multiple models with increasing \n",
    "# values of K, then choose the smallest K with validation score above some \n",
    "# threshold or some other criteria. \n",
    "# A good way to do this is loop over values of K and record the validation \n",
    "# scores for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2.4. L1 REGULARISATION FOR FEATURES SELECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def select_features_l1(X, y):\n",
    "    logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7, solver='liblinear').fit(X, y)\n",
    "    model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "    X_new = model.transform(X)\n",
    "\n",
    "    # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "    selected_features = pd.DataFrame(model.inverse_transform(X_new),\n",
    "                                     index=X.index,\n",
    "                                     columns=X.columns)\n",
    "\n",
    "    # Dropped columns have values of all 0s, keep other columns\n",
    "    cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "    return cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9655039361169727\n"
     ]
    }
   ],
   "source": [
    "# Entrainement et scoring\n",
    "n_samples = 10000\n",
    "X, y = train[feature_cols][:n_samples], train['is_attributed'][:n_samples]\n",
    "selected = select_features_l1(X, y)\n",
    "\n",
    "dropped_columns = feature_cols.drop(selected)\n",
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1))\n",
    "# old_score_1 : Validation AUC score: 0.9658334271834417\n",
    "# old_score_2 : Validation AUC score: 0.9625481759576047\n",
    "# new_score_3 : Validation AUC score: 0.9655039361169727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2.5. FEATURE SELECTION WITH TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're using a tree-based model, using another tree-based model for \n",
    "# feature selection might produce better results. What would you do \n",
    "# different to select the features using a trees classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could use something like RandomForestClassifier or \n",
    "# ExtraTreesClassifier to find feature importances. \n",
    "# SelectFromModel can use the feature importances to find the best \n",
    "# features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4.2.6. TOP K FEAUTURES WITH L1 REGULARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you've set the regularization parameter C=0.1 which led to some \n",
    "# number of features being dropped. \n",
    "# However, by setting C you aren't able to choose a certain number of \n",
    "# features to keep. \n",
    "# What would you do to keep the top K important features using L1 \n",
    "# regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To select a certain number of features with L1 regularization, you need \n",
    "# to find the regularization parameter that leaves the desired number of \n",
    "# features. \n",
    "# To do this you can iterate over models with different regularization \n",
    "# parameters from low to high and choose the one that leaves K features. \n",
    "# Note that for the scikit-learn models C is the inverse of the \n",
    "# regularization strength."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
